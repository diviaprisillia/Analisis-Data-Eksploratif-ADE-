{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNQiSujBfjWj"
      },
      "source": [
        "# **Week 4 Assignment: Saliency Maps**\n",
        "\n",
        "Welcome to the final programming exercise of this course! For this week, your task is to adapt the [Cats vs Dogs](https://www.tensorflow.org/datasets/catalog/cats_vs_dogs) Class Activation Map ungraded lab (the second ungraded lab of this week) and make it generate saliency maps instead.\n",
        "\n",
        "As discussed in the lectures, a saliency map shows the pixels which greatly impacts the classification of an image.\n",
        "- This is done by getting the gradient of the loss with respect to changes in the pixel values, then plotting the results.\n",
        "- From there, you can see if your model is looking at the correct features when classifying an image.\n",
        "  - For example, if you're building a dog breed classifier, you should be wary if your saliency map shows strong pixels outside the dog itself (e.g. sky, grass, dog house, etc...).\n",
        "\n",
        "In this assignment you will be given prompts but less starter code to fill in in.\n",
        "- It's good practice for you to try and write as much of this code as you can from memory and from searching the web.\n",
        "- **Whenever you feel stuck**, please refer back to the labs of this week to see how to write the code. In particular, look at:\n",
        "  - **Ungraded Lab 2: Cats vs Dogs CAM**\n",
        "  - **Ungraded Lab 3: Saliency**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eASwKp8LosEp"
      },
      "outputs": [],
      "source": [
        "# Install packages for compatibility\n",
        "\n",
        "# NOTE: You can safely ignore errors about version incompatibility of\n",
        "# Colab-bundled packages (e.g. xarray, pydantic, etc.)\n",
        "\n",
        "!pip install tf-keras==2.15 --quiet\n",
        "!pip install tensorflow==2.15 --quiet\n",
        "!pip install keras==2.15 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDHISSfBq40T"
      },
      "source": [
        "### Download test files and weights\n",
        "\n",
        "Let's begin by first downloading files we will be using for this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Laatr1c6lr1w"
      },
      "outputs": [],
      "source": [
        "# Download the same test files from the Cats vs Dogs ungraded lab\n",
        "!wget -O cat1.jpg https://storage.googleapis.com/tensorflow-1-public/tensorflow-3-temp/MLColabImages/cat1.jpeg\n",
        "!wget -O cat2.jpg https://storage.googleapis.com/tensorflow-1-public/tensorflow-3-temp/MLColabImages/cat2.jpeg\n",
        "!wget -O catanddog.jpg https://storage.googleapis.com/tensorflow-1-public/tensorflow-3-temp/MLColabImages/catanddog.jpeg\n",
        "!wget -O dog1.jpg https://storage.googleapis.com/tensorflow-1-public/tensorflow-3-temp/MLColabImages/dog1.jpeg\n",
        "!wget -O dog2.jpg https://storage.googleapis.com/tensorflow-1-public/tensorflow-3-temp/MLColabImages/dog2.jpeg\n",
        "\n",
        "# Download prepared weights\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1kipXTxesGJKGY1B8uSPRvxROgOH90fih' -O 0_epochs.h5\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1oiV6tjy5k7h9OHGTQaf0Ohn3FmF-uOs1' -O 15_epochs.h5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g24L3lKwqb3E"
      },
      "source": [
        "### Import the required packages\n",
        "\n",
        "Please import:\n",
        "\n",
        "  * Tensorflow\n",
        "  * Tensorflow Datasets\n",
        "  * Numpy\n",
        "  * Matplotlib's PyPlot\n",
        "  * Keras Models API classes you will be using\n",
        "  * Keras layers you will be using\n",
        "  * OpenCV (cv2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X86LKLvpBO2S"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th4dA3I8-9Ue"
      },
      "source": [
        "### Download and prepare the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1hujOK9rDyU"
      },
      "source": [
        "#### Load Cats vs Dogs\n",
        "\n",
        "* Required: Use Tensorflow Datasets to fetch the `cats_vs_dogs` dataset.\n",
        "  * Use the first 80% of the *train* split of the said dataset to create your training set.\n",
        "  * Set the `as_supervised` flag to create `(image, label)` pairs.\n",
        "    \n",
        "* Optional: You can create validation and test sets from the remaining 20% of the *train* split of `cats_vs_dogs` (i.e. you already used 80% for the train set). This is if you intend to train the model beyond what is required for submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w5HNdoHBQv_"
      },
      "outputs": [],
      "source": [
        "# Load the Cats vs Dogs dataset using TensorFlow Datasets\n",
        "data, metadata = tfds.load('cats_vs_dogs', as_supervised=True, with_info=True)\n",
        "\n",
        "# Split the dataset\n",
        "train_data = data['train']\n",
        "total_size = metadata.splits['train'].num_examples\n",
        "\n",
        "# Define split proportions\n",
        "train_split = int(total_size * 0.8)\n",
        "val_split = total_size - train_split\n",
        "\n",
        "# Create train, validation, and test datasets\n",
        "train_set = train_data.take(train_split)\n",
        "remaining_set = train_data.skip(train_split)\n",
        "val_set = remaining_set.take(val_split // 2)\n",
        "test_set = remaining_set.skip(val_split // 2)\n",
        "\n",
        "# Preprocessing function to normalize images\n",
        "def preprocess(image, label):\n",
        "    image = tf.image.resize(image, (300, 300))\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n",
        "\n",
        "# Apply preprocessing\n",
        "train_set = train_set.map(preprocess).batch(32).shuffle(buffer_size=1000)\n",
        "val_set = val_set.map(preprocess).batch(32)\n",
        "test_set = test_set.map(preprocess).batch(32)\n",
        "\n",
        "# Display example images\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_set.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy())\n",
        "        plt.title(\"Cat\" if labels[i].numpy() == 0 else \"Dog\")\n",
        "        plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Dataset prepared: Train, Validation, and Test sets created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXp0mV5Rbo76"
      },
      "source": [
        "#### Create preprocessing function\n",
        "\n",
        "Define a function that takes in an image and label. This will:\n",
        "  * cast the image to float32\n",
        "  * normalize the pixel values to [0, 1]\n",
        "  * resize the image to 300 x 300\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRkrL2aK2_UZ"
      },
      "outputs": [],
      "source": [
        "def augmentimages(image, label):\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    image = tf.image.resize(image, (300, 300))\n",
        "    return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzvF61GV32k_"
      },
      "source": [
        "#### Preprocess the training set\n",
        "\n",
        "Use the `map()` and pass in the method that you just defined to preprocess the training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpNEfDKM353a"
      },
      "outputs": [],
      "source": [
        "augmented_training_data = train_set.map(augmentimages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4nFaMIMbrvA"
      },
      "source": [
        "#### Create batches of the training set.\n",
        "\n",
        "This is already provided for you. Normally, you will want to shuffle the training set. But for predictability in the grading, we will simply create the batches.\n",
        "\n",
        "```Python\n",
        "# Shuffle the data if you're working on your own personal project\n",
        "train_batches = augmented_training_data.shuffle(1024).batch(32)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POhDDPBY3vnL"
      },
      "outputs": [],
      "source": [
        "train_batches = augmented_training_data.batch(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za5HxgT1_Cw6"
      },
      "source": [
        "### Build the Cats vs Dogs classifier\n",
        "\n",
        "You'll define a model that is nearly the same as the one in the Cats vs. Dogs CAM lab.\n",
        "* Please preserve the architecture of the model in the Cats vs Dogs CAM lab (this week's second lab) except for the final `Dense` layer.\n",
        "* You should modify the Cats vs Dogs model at the last dense layer to output 2 neurons instead of 1.\n",
        "  - This is because you will adapt the `do_salience()` function from the lab and that works with one-hot encoded labels.\n",
        "  - You can do this by changing the `units` argument of the output Dense layer from 1 to 2, with one for each of the classes (i.e. cats and dogs).\n",
        "  - You should choose an activation that outputs a probability for each of the 2 classes (i.e. categories), where the sum of the probabilities adds up to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoyCA80GBSlG"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential([\n",
        "    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(300, 300, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(2, activation='softmax')  # Modified output layer for 2 classes\n",
        "])\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktnATyllHXC4"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```txt\n",
        "Model: \"sequential\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "conv2d (Conv2D)              (None, 300, 300, 16)      448       \n",
        "_________________________________________________________________\n",
        "max_pooling2d (MaxPooling2D) (None, 150, 150, 16)      0         \n",
        "_________________________________________________________________\n",
        "conv2d_1 (Conv2D)            (None, 150, 150, 32)      4640      \n",
        "_________________________________________________________________\n",
        "max_pooling2d_1 (MaxPooling2 (None, 75, 75, 32)        0         \n",
        "_________________________________________________________________\n",
        "conv2d_2 (Conv2D)            (None, 75, 75, 64)        18496     \n",
        "_________________________________________________________________\n",
        "max_pooling2d_2 (MaxPooling2 (None, 37, 37, 64)        0         \n",
        "_________________________________________________________________\n",
        "conv2d_3 (Conv2D)            (None, 37, 37, 128)       73856     \n",
        "_________________________________________________________________\n",
        "global_average_pooling2d (Gl (None, 128)               0         \n",
        "_________________________________________________________________\n",
        "dense (Dense)                (None, 2)                 258       \n",
        "=================================================================\n",
        "Total params: 97,698\n",
        "Trainable params: 97,698\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6nou82P_b5d"
      },
      "source": [
        "### Create a function to generate the saliency map\n",
        "\n",
        "Complete the `do_salience()` function below to save the **normalized_tensor** image.\n",
        "- The major steps are listed as comments below.\n",
        "  - Each section may involve multiple lines of code.\n",
        "- Try your best to write the code from memory or by performing web searches.\n",
        "  - Whenever you get stuck, you can review the \"saliency\" lab (the third lab of this week) to help remind you of what code to write"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKbvh3bl9vnG"
      },
      "outputs": [],
      "source": [
        "def do_salience(image_path, model, label, prefix):\n",
        "    '''\n",
        "    Generates the saliency map of a given image.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file\n",
        "        model (keras.Model): Cats vs Dogs classifier model\n",
        "        label (int): Ground truth label of the image (0 for Cat, 1 for Dog)\n",
        "        prefix (str): Prefix for saving the saliency map image\n",
        "    '''\n",
        "    # Read and preprocess the image\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = tf.image.resize(image, (300, 300)) / 255.0\n",
        "    image = tf.expand_dims(image, axis=0)\n",
        "\n",
        "    # One-hot encode the label\n",
        "    num_classes = 2\n",
        "    expected_output = tf.one_hot([label], num_classes)\n",
        "\n",
        "    # Compute the gradients\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(image)\n",
        "        predictions = model(image)\n",
        "        loss = tf.keras.losses.categorical_crossentropy(expected_output, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, image)\n",
        "\n",
        "    # Generate the saliency map\n",
        "    grayscale_tensor = tf.reduce_max(tf.abs(gradients), axis=-1)\n",
        "    normalized_tensor = 255 * (grayscale_tensor - tf.reduce_min(grayscale_tensor)) / (tf.reduce_max(grayscale_tensor) - tf.reduce_min(grayscale_tensor))\n",
        "    normalized_tensor = tf.cast(normalized_tensor, tf.uint8)\n",
        "    normalized_tensor = tf.squeeze(normalized_tensor)\n",
        "\n",
        "    # Plot and save the saliency map\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(normalized_tensor, cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "    salient_image_name = f\"{prefix}_{image_path.split('/')[-1]}\"\n",
        "    normalized_tensor = tf.expand_dims(normalized_tensor, -1)\n",
        "    normalized_tensor = tf.io.encode_jpeg(normalized_tensor, quality=100, format='grayscale')\n",
        "    tf.io.write_file(salient_image_name, normalized_tensor)\n",
        "    print(f\"Saliency map saved as {salient_image_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "li1idRy-parp"
      },
      "source": [
        "### Generate saliency maps with untrained model\n",
        "\n",
        "As a sanity check, you will load initialized (i.e. untrained) weights and use the function you just implemented.\n",
        "- This will check if you built the model correctly and are able to create a saliency map.\n",
        "\n",
        "If an error pops up when loading the weights or the function does not run, please check your implementation for bugs.\n",
        "- You can check the ungraded labs of this week.\n",
        "\n",
        "Please apply your `do_salience()` function on the following image files:\n",
        "\n",
        "* `cat1.jpg`\n",
        "* `cat2.jpg`\n",
        "* `catanddog.jpg`\n",
        "* `dog1.jpg`\n",
        "* `dog2.jpg`\n",
        "\n",
        "Cats will have the label `0` while dogs will have the label `1`.\n",
        "- For the catanddog, please use `0`.\n",
        "- For the prefix of the salience images that will be generated, please use the prefix `epoch0_salient`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k39fF4n8fgG0"
      },
      "outputs": [],
      "source": [
        "# Load initial (untrained) weights\n",
        "model.load_weights('0_epochs.h5')\n",
        "\n",
        "# Generate saliency maps for the test images with untrained weights\n",
        "image_files = ['cat1.jpg', 'cat2.jpg', 'catanddog.jpg', 'dog1.jpg', 'dog2.jpg']\n",
        "labels = [0, 0, 0, 1, 1]  # 0 for Cats, 1 for Dogs (catanddog.jpg is treated as a Cat)\n",
        "\n",
        "# Iterate over the images and generate saliency maps\n",
        "for image_path, label in zip(image_files, labels):\n",
        "    do_salience(image_path, model, label, prefix=\"epoch0_salient\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kcdyut5E2Tk"
      },
      "source": [
        "With untrained weights, you will see something like this in the output.\n",
        "- You will see strong pixels outside the cat that the model uses that when classifying the image.\n",
        "- After training that these will slowly start to localize to features inside the pet.\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1h5wP52lwbBUMVLlsgyb-tQl_I9eu42X7' alt='saliency'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZhZgd0x_JvN"
      },
      "source": [
        "### Configure the model for training\n",
        "\n",
        "Use `model.compile()` to define the loss, metrics and optimizer.\n",
        "\n",
        "* Choose a loss function for the model to use when training.\n",
        "  - For `model.compile()` the ground truth labels from the training set are passed to the model as **integers** (i.e. 0 or 1) as opposed to one-hot encoded vectors.\n",
        "  - The model predictions are class probabilities.\n",
        "  - You can browse the [tf.keras.losses](https://www.tensorflow.org/api_docs/python/tf/keras/losses) and determine which one is best used for this case.\n",
        "  - Remember that you can pass the function as a string (e.g. `loss = 'loss_function_a'`).\n",
        "\n",
        "* For metrics, you can measure `accuracy`.\n",
        "* For the optimizer, please use [RMSProp](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop).\n",
        "  - Please use the default learning rate of `0.001`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkyWZ5KdBo-z"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',  # Loss function for integer labels\n",
        "    optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),  # RMSProp optimizer with default learning rate\n",
        "    metrics=['accuracy']  # Metric to track during training\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otIoJJw7_ZFN"
      },
      "source": [
        "### Train your model\n",
        "\n",
        "Please pass in the training batches and train your model for just **3** epochs.\n",
        "- **Note:** Please do not exceed 3 epochs because the grader will expect 3 epochs when grading your output.\n",
        "  - After submitting your zipped folder for grading, feel free to continue training to improve your model.\n",
        "\n",
        "We have loaded pre-trained weights for 15 epochs so you can get a better output when you visualize the saliency maps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YSNp7k7BqfL"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained weights for 15 epochs\n",
        "model.load_weights('15_epochs.h5')\n",
        "\n",
        "# Train the model for just 3 epochs\n",
        "history = model.fit(\n",
        "    train_set,  # Training dataset\n",
        "    validation_data=val_set,  # Validation dataset\n",
        "    epochs=3,  # Train for 3 epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tTqtLN3tQJx"
      },
      "source": [
        "### Generate saliency maps at 18 epochs\n",
        "\n",
        "You will now use your `do_salience()` function again on the same test images. Please use the same parameters as before but this time, use the prefix `salient`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXFtabyVhIKN"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGTFcfEgM6aV"
      },
      "source": [
        "You should see that the strong pixels are now very less than the ones you generated earlier. Moreover, most of them are now found on features within the pet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPtx-u4u_jL5"
      },
      "source": [
        "### Zip the images for grading\n",
        "\n",
        "Please run the cell below to zip the normalized tensor images you generated at 18 epochs. If you get an error, please check that you have files named:\n",
        "\n",
        "* salientcat1.jpg\n",
        "* salientcat2.jpg\n",
        "* salientcatanddog.jpg\n",
        "* salientdog1.jpg\n",
        "* salientdog2.jpg\n",
        "\n",
        "Afterwards, please download the **images.zip** from the Files bar on the left."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-MhcA8Uh8H_"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "!rm images.zip\n",
        "\n",
        "filenames = ['cat1.jpg', 'cat2.jpg', 'catanddog.jpg', 'dog1.jpg', 'dog2.jpg']\n",
        "\n",
        "# writing files to a zipfile\n",
        "with ZipFile('images.zip','w') as zip:\n",
        "  for file in filenames:\n",
        "    zip.write('salient' + file)\n",
        "\n",
        "print(\"images.zip generated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMOgx-N55A6p"
      },
      "source": [
        "### Optional: Saliency Maps at 95 epochs\n",
        "\n",
        "We have pre-trained weights generated at 95 epochs and you can see the difference between the maps you generated at 18 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elUfhSmMvJZh"
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=14vFpBJsL_TNQeugX8vUTv8dYZxn__fQY' -O 95_epochs.h5\n",
        "\n",
        "model.load_weights('95_epochs.h5')\n",
        "\n",
        "do_salience('cat1.jpg', model, 0, \"epoch95_salient\")\n",
        "do_salience('cat2.jpg', model, 0, \"epoch95_salient\")\n",
        "do_salience('catanddog.jpg', model, 0, \"epoch95_salient\")\n",
        "do_salience('dog1.jpg', model, 1, \"epoch95_salient\")\n",
        "do_salience('dog2.jpg', model, 1, \"epoch95_salient\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuKLdQhvAaTd"
      },
      "source": [
        "**Congratulations on completing this week's assignment! Please go back to the Coursera classroom and upload the zipped folder to be graded.**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}